{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Data Science\n",
    "\n",
    "### Predictive Analysis - numerical data with regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspired by [this repo](https://github.com/Weesper1985/Predict_real_estate_prices), codes [here](https://www.kaggle.com/dhainjeamita/linear-regression/code), [here](https://www.kaggle.com/vikrishnan/house-sales-price-using-regression) and [here](https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/). Used [this data](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime). Tried using [feature selection](http://scikit-learn.org/stable/modules/feature_selection.html) techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab\n",
    "from itertools import cycle\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, RidgeCV, ElasticNet\n",
    "from sklearn.linear_model import lasso_path, enet_path\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold, cross_val_score, validation_curve\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"theano\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "##Turning GPU invisible\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "#!export CUDA_VISIBLE_DEVICES=\"\"\n",
    "##Turning GPU visible again\n",
    "#!unset CUDA_VISIBLE_DEVICES\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "#matplotlib.rcdefaults()\n",
    "#matplotlib.verbose.set_level('silent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 16697914926440138365]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking if we have a GPU\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the datasets and exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../datasets/CSVs/\"\n",
    "outputs = \"../outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/CSVs/communities.data  ../datasets/CSVs/communities.names\n"
     ]
    }
   ],
   "source": [
    "! ls ../datasets/CSVs/communities.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version https://git-lfs.github.com/spec/v1\n",
      "oid sha256:80914f24ab37261fb0fbda7e417cad8001335dadda4a8d702c265601ef8434fc\n",
      "size 27179\n"
     ]
    }
   ],
   "source": [
    "! head -n10 ../datasets/CSVs/communities.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version https://git-lfs.github.com/spec/v1\n",
      "oid sha256:09e0b5c07eae24c1efab19b2edee05e160e7f5743b6f31e31eec3d73624da2ea\n",
      "size 1102815\n"
     ]
    }
   ],
   "source": [
    "! head -n5 ../datasets/CSVs/communities.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "names = open(os.path.join(datapath, 'communities.names'), 'r').readlines()\n",
    "attributes = [n for n in names if n.startswith('@attribute')]\n",
    "attributes = [a.split()[1] for a in attributes]\n",
    "print(len(attributes))\n",
    "print(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>version https://git-lfs.github.com/spec/v1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oid sha256:09e0b5c07eae24c1efab19b2edee05e160e7f5743b6f31e31eec3d73624da2ea</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size 1102815</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [version https://git-lfs.github.com/spec/v1, oid sha256:09e0b5c07eae24c1efab19b2edee05e160e7f5743b6f31e31eec3d73624da2ea, size 1102815]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(datapath, 'communities.data'), \n",
    "                 na_values='?', \n",
    "                 header=None, \n",
    "                 names=attributes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3 entries, version https://git-lfs.github.com/spec/v1 to size 1102815\n",
      "Empty DataFrame"
     ]
    }
   ],
   "source": [
    "df.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['state' 'county' 'community' 'communityname' 'fold'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f23c8784094d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             'LemasGangUnitDeploy', 'PolicBudgPerPop', 'OtherPerCap']\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3695\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3696\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3697\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3699\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3111\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3141\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3143\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3144\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4403\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4404\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4405\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4406\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['state' 'county' 'community' 'communityname' 'fold'] not found in axis\""
     ]
    }
   ],
   "source": [
    "#non numeric\n",
    "columns1 = ['state', 'county', 'community', 'communityname', 'fold']\n",
    "\n",
    "#many missing values\n",
    "columns2 = ['LemasSwornFT', 'LemasSwFTPerPop', 'LemasSwFTFieldOps', 'LemasSwFTFieldPerPop', \n",
    "            'LemasTotalReq',  'LemasTotReqPerPop', 'PolicReqPerOffic', 'PolicPerPop', \n",
    "            'RacialMatchCommPol', 'PctPolicWhite', 'PctPolicBlack', 'PctPolicHisp', \n",
    "            'PctPolicAsian', 'PctPolicMinor', 'OfficAssgnDrugUnits', 'NumKindsDrugsSeiz',\n",
    "            'PolicAveOTWorked', 'PolicCars', 'PolicOperBudg', 'LemasPctPolicOnPatr',\n",
    "            'LemasGangUnitDeploy', 'PolicBudgPerPop', 'OtherPerCap']\n",
    "\n",
    "df.drop(columns=columns1, inplace=True)\n",
    "df.drop(columns=columns2, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms\n",
    "df.hist(bins=10,figsize=(18,14),grid=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='box', subplots=True, layout=(10,10), sharex=False, sharey=False, figsize=(16,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation\n",
    "pd.set_option('precision', 2)\n",
    "df.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=df.corr()\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(corr, vmax=.8, linewidths=0.01,square=True,annot=False,cmap='YlGnBu',linecolor=\"white\")\n",
    "plt.title('Correlation between features');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes = df['ViolentCrimesPerPop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, col in enumerate(df.columns[0:6]):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    x = df[col]\n",
    "    y = crimes\n",
    "    plt.plot(x, y, 'o')\n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, col in enumerate(df.columns[6:12]):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    x = df[col]\n",
    "    y = crimes\n",
    "    plt.plot(x, y, 'o')\n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, col in enumerate(df.columns[12:18]):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    x = df[col]\n",
    "    y = crimes\n",
    "    plt.plot(x, y, 'o')\n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, col in enumerate(df.columns[18:24]):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    x = df[col]\n",
    "    y = crimes\n",
    "    plt.plot(x, y, 'o')\n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, col in enumerate(df.columns[24:30]):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    x = df[col]\n",
    "    y = crimes\n",
    "    plt.plot(x, y, 'o')\n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, col in enumerate(df.columns[30:36]):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    x = df[col]\n",
    "    y = crimes\n",
    "    plt.plot(x, y, 'o')\n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, col in enumerate(df.columns[36:42]):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    x = df[col]\n",
    "    y = crimes\n",
    "    plt.plot(x, y, 'o')\n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, col in enumerate(df.columns[42:48]):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    x = df[col]\n",
    "    y = crimes\n",
    "    plt.plot(x, y, 'o')\n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, col in enumerate(df.columns[48:54]):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    x = df[col]\n",
    "    y = crimes\n",
    "    plt.plot(x, y, 'o')\n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, col in enumerate(df.columns[54:60]):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    x = df[col]\n",
    "    y = crimes\n",
    "    plt.plot(x, y, 'o')\n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, col in enumerate(df.columns[60:66]):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    x = df[col]\n",
    "    y = crimes\n",
    "    plt.plot(x, y, 'o')\n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, col in enumerate(df.columns[66:72]):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    x = df[col]\n",
    "    y = crimes\n",
    "    plt.plot(x, y, 'o')\n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, col in enumerate(df.columns[72:78]):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    x = df[col]\n",
    "    y = crimes\n",
    "    plt.plot(x, y, 'o')\n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, col in enumerate(df.columns[78:84]):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    x = df[col]\n",
    "    y = crimes\n",
    "    plt.plot(x, y, 'o')\n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, col in enumerate(df.columns[84:90]):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    x = df[col]\n",
    "    y = crimes\n",
    "    plt.plot(x, y, 'o')\n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, col in enumerate(df.columns[90:96]):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    x = df[col]\n",
    "    y = crimes\n",
    "    plt.plot(x, y, 'o')\n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, col in enumerate(df.columns[96:]):\n",
    "    plt.subplot(1, 7, i+1)\n",
    "    x = df[col]\n",
    "    y = crimes\n",
    "    plt.plot(x, y, 'o')\n",
    "    # Create regression line\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('crimes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point here is to test 'crimes' in a very lean way. \n",
    "We'll do this paying attention to: Histogram - Kurtosis and skewness. \n",
    "Normal probability plot - Data distribution should closely follow the diagonal that represents the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#histogram and normal probability plot\n",
    "sns.distplot(crimes, hist=True);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(crimes, plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Data\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = df.values[:,0:-1]\n",
    "y = df.values[:,99]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-out validation dataset\n",
    "validation_size = 0.20\n",
    "seed = 0\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric using Root Mean Square error method\n",
    "#http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "num_folds = 10\n",
    "#score = 'neg_mean_squared_error'\n",
    "score = 'r2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LinearRegression()))\n",
    "models.append(('LASSO', Lasso(random_state=seed)))\n",
    "models.append(('EN', ElasticNet(random_state=seed)))\n",
    "models.append(('KNN', KNeighborsRegressor()))\n",
    "models.append(('CART', DecisionTreeRegressor(random_state=seed)))\n",
    "models.append(('SVR', SVR()))\n",
    "models.append(('RIDGE', Ridge(random_state=seed)))\n",
    "models.append(('RIDGECV', RidgeCV()))\n",
    "models.append(('MLPR', MLPRegressor(random_state=seed)))\n",
    "models.append(('GPR', GaussianProcessRegressor(random_state=seed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=score)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LinearRegression())])))\n",
    "pipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso(random_state=seed))])))\n",
    "pipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet(random_state=seed))])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor(random_state=seed))])))\n",
    "pipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()),('SVR', SVR())])))\n",
    "pipelines.append(('ScaledRIDGE', Pipeline([('Scaler', StandardScaler()),('RIDGE', Ridge(random_state=seed))])))\n",
    "pipelines.append(('ScaledRIDGECV', Pipeline([('Scaler', StandardScaler()),('RIDGECV', RidgeCV())])))\n",
    "pipelines.append(('ScaledMLPR', Pipeline([('Scaler', StandardScaler()),('MLPR', MLPRegressor(random_state=seed))])))\n",
    "pipelines.append(('ScaledGPR', Pipeline([('Scaler', StandardScaler()),('GPR', GaussianProcessRegressor(random_state=seed))])))\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=score)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "fig.suptitle('Scaled Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Algorithm tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "k_values = np.array([1,3,5,7,9,11,13,15,17,19,21])\n",
    "param_grid = dict(n_neighbors=k_values)\n",
    "model = KNeighborsRegressor()\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=score, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Tree Algorithm tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "\n",
    "min_samples_split_values = [2,3,4]\n",
    "min_samples_leaf_values = [1,20,40,60,80]\n",
    "\n",
    "param_grid = dict(min_samples_split=min_samples_split_values, min_samples_leaf=min_samples_leaf_values)\n",
    "model = DecisionTreeRegressor(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=score, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Algorithm Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "alpha_values = np.linspace(0.0004,0.002,15)\n",
    "param_grid = dict(alpha=alpha_values)\n",
    "model = Lasso(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=score, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Algorithm Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "alpha_values = [0, 1, 10, 20, 50, 80, 100, 120, 200, 500]\n",
    "param_grid = dict(alpha=alpha_values)\n",
    "model = Ridge(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=score, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ElasticNet Algorithm Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "l1_values = np.linspace(1e-07,1e-05, 15)\n",
    "param_grid = dict(l1_ratio=l1_values)\n",
    "model = ElasticNet(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=score, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP Algorithm Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "\n",
    "solver_values = ['lbfgs'] #, 'sgd', 'adam']\n",
    "alpha_values = np.linspace(1e-5, 1e-4, 10)\n",
    "\n",
    "param_grid = dict(solver=solver_values, alpha=alpha_values)\n",
    "model = MLPRegressor(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=score, cv=kfold)\n",
    "\n",
    "#grid_result = grid.fit(rescaledX, Y_train)\n",
    "grid_result = grid.fit(X_train, Y_train) #Without scaling, that seems not to improve model\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPR Algorithm Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "\n",
    "alpha_values = np.linspace(1e-8, 1e-9, 6)\n",
    "kernel_values = [1.0 * RBF(length_scale=100.0, length_scale_bounds=(1e-2, 1e3)) + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1)),\n",
    "                 1.0 * RBF(length_scale=100.0, length_scale_bounds=(1e-2, 1e3)) + WhiteKernel(noise_level=2, noise_level_bounds=(1e-10, 1e+1))]\n",
    "\n",
    "param_grid = dict(kernel=kernel_values, alpha=alpha_values)\n",
    "model = GaussianProcessRegressor(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=score, cv=kfold)\n",
    "\n",
    "#grid_result = grid.fit(rescaledX, Y_train)\n",
    "grid_result = grid.fit(X_train, Y_train) #Without scaling, that seems not to improve model\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the tunned algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = []\n",
    "pipelines.append(('Scaled LR', Pipeline([('Scaler', StandardScaler()),('LR', LinearRegression())])))\n",
    "pipelines.append(('Scaled Opt LASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso(alpha=0.0012, random_state=seed))])))\n",
    "pipelines.append(('Scaled Opt EN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet(l1_ratio=1e-07, random_state=seed))])))\n",
    "pipelines.append(('Scaled Opt KNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor(n_neighbors=11))])))\n",
    "pipelines.append(('Scaled Opt CART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor(min_samples_leaf=40,\n",
    "                                                                                                           min_samples_split=3,\n",
    "                                                                                                           random_state=seed))])))\n",
    "pipelines.append(('Scaled SVR', Pipeline([('Scaler', StandardScaler()),('SVR', SVR())])))\n",
    "pipelines.append(('Scaled Opt RIDGE', Pipeline([('Scaler', StandardScaler()),('RIDGE', Ridge(alpha=80, random_state=seed))])))\n",
    "pipelines.append(('Scaled RIDGECV', Pipeline([('Scaler', StandardScaler()),('RIDGECV', RidgeCV())])))\n",
    "pipelines.append(('Opt MLPR', Pipeline([('MLPR', MLPRegressor(alpha=5e-05, solver='lbfgs', random_state=seed))])))\n",
    "pipelines.append(('Opt GPR', Pipeline([('GPR', GaussianProcessRegressor(alpha=1e-8, \n",
    "                                                                        kernel=1.0 * RBF(length_scale=100.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
    "                                                                        + WhiteKernel(noise_level=2, noise_level_bounds=(1e-10, 1e+1)), \n",
    "                                                                        random_state=seed))])))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=score)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "fig.suptitle('Scaled and Optimized Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembles = []\n",
    "ensembles.append(('Scaled AB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostRegressor(random_state=seed))])))\n",
    "ensembles.append(('Scaled GBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor(random_state=seed))])))\n",
    "ensembles.append(('Scaled RF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestRegressor(random_state=seed))])))\n",
    "ensembles.append(('Scaled ET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesRegressor(random_state=seed))])))\n",
    "results_ensembles = []\n",
    "names_ensembles = []\n",
    "for name, model in ensembles:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=score)\n",
    "    results_ensembles.append(cv_results)\n",
    "    names_ensembles.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "fig.suptitle('Scaled Ensemble Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results_ensembles)\n",
    "ax.set_xticklabels(names_ensembles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost Algorithm Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "param_grid = dict(n_estimators=np.array([2,5,10,20,30,40]))\n",
    "model = AdaBoostRegressor(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=score, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBM Algorithm Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "param_grid = dict(n_estimators=np.array([20,35,40,45,50,55,60,100]))\n",
    "model = GradientBoostingRegressor(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=score, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Algorithm Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "\n",
    "n_estimators_values=np.array([30,50,70])\n",
    "min_samples_split_values = [2,3]\n",
    "min_samples_leaf_values = [10,20,30]\n",
    "\n",
    "param_grid = dict(n_estimators=n_estimators_values,\n",
    "                  min_samples_split=min_samples_split_values, \n",
    "                  min_samples_leaf=min_samples_leaf_values)\n",
    "\n",
    "model = RandomForestRegressor(random_state=seed, n_jobs=-1)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, scoring=score, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Tree Algorithm Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "\n",
    "n_estimators_values=np.array([200,300])\n",
    "min_samples_split_values = [2,3]\n",
    "min_samples_leaf_values = [1,10,20]\n",
    "\n",
    "param_grid = dict(n_estimators=n_estimators_values,\n",
    "                  min_samples_split=min_samples_split_values, \n",
    "                  min_samples_leaf=min_samples_leaf_values)\n",
    "\n",
    "model = ExtraTreesRegressor(random_state=seed, n_jobs=-1)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, scoring=score, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimized Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembles = []\n",
    "ensembles.append(('Scaled Opt AB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostRegressor(n_estimators=5))])))\n",
    "ensembles.append(('Scaled Opt GBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor(n_estimators=60))])))\n",
    "ensembles.append(('Scaled Opt RF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestRegressor(n_estimators=70, \n",
    "                                                                                                        min_samples_split=3,\n",
    "                                                                                                        min_samples_leaf=10))])))\n",
    "ensembles.append(('Scaled Opt ET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesRegressor(n_estimators=300, \n",
    "                                                                                                      min_samples_split=3,\n",
    "                                                                                                      min_samples_leaf=1))])))\n",
    "results_ensembles = []\n",
    "names_ensembles = []\n",
    "for name, model in ensembles:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=score)\n",
    "    results_ensembles.append(cv_results)\n",
    "    names_ensembles.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "fig.suptitle('Scaled Optimized Ensemble Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results_ensembles)\n",
    "ax.set_xticklabels(names_ensembles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing all the optimized results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = results + results_ensembles\n",
    "names_all = names + names_ensembles\n",
    "\n",
    "pd.Series([x.mean() for x in results_all], index=names_all).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,12))\n",
    "fig.suptitle('All Algorithms Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "#plt.boxplot(results, widths=0.5)\n",
    "sns.boxplot(data=results_all)\n",
    "ax.set_xticklabels(names_all)\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our best regressors are Gaussian Processes, Extra Trees, Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how Lasso / Ridge / ElasticNet Regularization  \n",
    "i.e. elimination of irrelevant features setting their coeficients to Zero or close:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "model = Lasso(alpha=0.00145, random_state=seed)\n",
    "model.fit(rescaledX, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = pd.Series(model.coef_, index=df.columns[:-1]).sort_values()\n",
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval.plot(kind='bar', title='Modal Coefficients', figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval2 = eval[np.abs(eval)>1e-6]\n",
    "eval2.plot(kind='bar', title='Modal Coefficients', figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying the Lasso and ElasticNet paths\n",
    "\n",
    "(see pages pages 69-73 from [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 5e-3\n",
    "\n",
    "print(\"Computing regularization path using the lasso...\")\n",
    "alphas_lasso, coefs_lasso, _ = lasso_path(rescaledX, Y_train, eps, random_state=0)\n",
    "\n",
    "print(\"Computing regularization path using the positive lasso...\")\n",
    "alphas_positive_lasso, coefs_positive_lasso, _ = lasso_path(rescaledX, Y_train, eps, positive=True, fit_intercept=False)\n",
    "\n",
    "print(\"Computing regularization path using the elastic net...\")\n",
    "alphas_enet, coefs_enet, _ = enet_path(rescaledX, Y_train, eps=eps, l1_ratio=0.8, fit_intercept=False)\n",
    "\n",
    "print(\"Computing regularization path using the positive elastic net...\")\n",
    "alphas_positive_enet, coefs_positive_enet, _ = enet_path(rescaledX, Y_train, eps=eps, l1_ratio=0.8, positive=True, fit_intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "plt.figure(figsize=(12,8))\n",
    "ax = plt.gca()\n",
    "\n",
    "colors = cycle(['b', 'r', 'g', 'c', 'k'])\n",
    "neg_log_alphas_lasso = -np.log10(alphas_lasso)\n",
    "neg_log_alphas_enet = -np.log10(alphas_enet)\n",
    "for coef_l, coef_e, c in zip(coefs_lasso, coefs_enet, colors):\n",
    "    l1 = plt.plot(neg_log_alphas_lasso, coef_l, c=c)\n",
    "    l2 = plt.plot(neg_log_alphas_enet, coef_e, linestyle='--', c=c)\n",
    "\n",
    "plt.xlabel('-Log(alpha)')\n",
    "plt.ylabel('coefficients')\n",
    "plt.title('Lasso and Elastic-Net Paths')\n",
    "plt.legend((l1[-1], l2[-1]), ('Lasso', 'Elastic-Net'), loc='lower left')\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "ax = plt.gca()\n",
    "neg_log_alphas_positive_lasso = -np.log10(alphas_positive_lasso)\n",
    "for coef_l, coef_pl, c in zip(coefs_lasso, coefs_positive_lasso, colors):\n",
    "    l1 = plt.plot(neg_log_alphas_lasso, coef_l, c=c)\n",
    "    l2 = plt.plot(neg_log_alphas_positive_lasso, coef_pl, linestyle='--', c=c)\n",
    "\n",
    "plt.xlabel('-Log(alpha)')\n",
    "plt.ylabel('coefficients')\n",
    "plt.title('Lasso and positive Lasso')\n",
    "plt.legend((l1[-1], l2[-1]), ('Lasso', 'positive Lasso'), loc='lower left')\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "ax = plt.gca()\n",
    "neg_log_alphas_positive_enet = -np.log10(alphas_positive_enet)\n",
    "for (coef_e, coef_pe, c) in zip(coefs_enet, coefs_positive_enet, colors):\n",
    "    l1 = plt.plot(neg_log_alphas_enet, coef_e, c=c)\n",
    "    l2 = plt.plot(neg_log_alphas_positive_enet, coef_pe, linestyle='--', c=c)\n",
    "\n",
    "plt.xlabel('-Log(alpha)')\n",
    "plt.ylabel('coefficients')\n",
    "plt.title('Elastic-Net and positive Elastic-Net')\n",
    "plt.legend((l1[-1], l2[-1]), ('Elastic-Net', 'positive Elastic-Net'),\n",
    "           loc='lower left')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the validation dataset\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "model = Lasso(alpha=0.00145, random_state=0)\n",
    "model.fit(rescaledX, Y_train)\n",
    "\n",
    "rescaledValidationX = scaler.transform(X_validation)\n",
    "predictions = model.predict(rescaledValidationX)\n",
    "print(mean_squared_error(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying Keras Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing $R^2$ metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coeff_determination(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(13, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error', metrics=[coeff_determination])\n",
    "model.summary()\n",
    "\n",
    "# Fit the model\n",
    "early_stopping = EarlyStopping(monitor='coeff_determination', patience=10)\n",
    "history =  model.fit(X_train, \n",
    "                     Y_train, \n",
    "                     epochs=150, \n",
    "                     batch_size=50, \n",
    "                     verbose=0, \n",
    "                     validation_data=(X_validation, Y_validation),) \n",
    "#                     callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(history.history)\n",
    "hist_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_validation, Y_validation, verbose=1)\n",
    "print(\"%s: %.4f%%\" % (model.metrics_names[0], scores[0]))\n",
    "print(\"%s: %.4f%%\" % (model.metrics_names[1], scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,6))\n",
    "plt.style.use('bmh')\n",
    "params_dict = dict(linestyle='solid', linewidth=0.25, marker='o', markersize=6)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(hist_df.loss, label='Training loss', **params_dict)\n",
    "plt.plot(hist_df.val_loss, label='Validation loss', **params_dict)\n",
    "plt.title('Loss for ' + str(len(history.epoch)) + ' epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(hist_df.coeff_determination, label='Training R2', **params_dict)\n",
    "plt.plot(hist_df.val_coeff_determination, label='Validation R2', **params_dict)\n",
    "plt.title('R2 for ' + str(len(history.epoch)) + ' epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('R2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "y_pred = model.predict(X_validation)\n",
    "y_pred[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using k-fold cross validation\n",
    "i=1\n",
    "cvscores0 = []\n",
    "cvscores1 = []\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "for train, validation in kfold.split(X, y):\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=X[train].shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(optimizer='sgd', loss='mean_squared_error', metrics=[coeff_determination])\n",
    "    # Fit the model\n",
    "    model.fit(X[train], \n",
    "              y[train], \n",
    "              epochs=150, \n",
    "              batch_size=20, \n",
    "              verbose=0, \n",
    "              validation_data=(X[validation], y[validation]),) \n",
    "              #callbacks=[early_stopping])\n",
    "    # evaluate the model\n",
    "    print('\\nResults from #{} run...'.format(i))\n",
    "    scores = model.evaluate(X[validation], y[validation], verbose=1)\n",
    "    print(\"%s: %.4f%%\" % (model.metrics_names[0], scores[0]))\n",
    "    print(\"%s: %.4f%%\" % (model.metrics_names[1], scores[1]))\n",
    "    cvscores0.append(scores[0])\n",
    "    cvscores1.append(scores[1])\n",
    "    i+=1\n",
    "model.summary()\n",
    "print(\"%.2f%% (+/- %.4f%%)\" % (np.mean(cvscores0), np.std(cvscores0)))\n",
    "print(\"%.2f%% (+/- %.4f%%)\" % (np.mean(cvscores1), np.std(cvscores1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
