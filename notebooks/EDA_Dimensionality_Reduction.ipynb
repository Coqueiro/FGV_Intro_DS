{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Data Science\n",
    "\n",
    "### Exploratory Data Analysis - Dimensionality Reduction\n",
    "\n",
    "based on [this](https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/) post  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display, Image\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../datasets/\"\n",
    "outputs = \"../outputs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "\n",
    "Have you ever worked on a dataset with more than a thousand features? How about over 50,000 features? I have, and let me tell you it’s a very challenging task, especially if you don’t know where to start! Having a high number of variables is both a boon and a curse. It’s great that we have loads of data for analysis, but it is challenging due to size.  \n",
    "\n",
    "It’s not feasible to analyze each and every variable at a microscopic level. It might take us days or months to perform any meaningful analysis and we’ll lose a ton of time and money for our business! Not to mention the amount of computational power this will take. We need a better way to deal with high dimensional data so that we can quickly extract patterns and insights from it. So how do we approach such a dataset?  \n",
    "Using dimensionality reduction techniques, of course. You can use this concept to reduce the number of features in your dataset without having to lose much information and keep (or improve) the model’s performance. It’s a really powerful way to deal with huge datasets.\n",
    "\n",
    "This is a comprehensive guide to various dimensionality reduction techniques that can be used in practical scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents  \n",
    "\n",
    "+ What is Dimensionality Reduction?  \n",
    "+ Why is Dimensionality Reduction required?  \n",
    "+ Common Dimensionality Reduction Techniques  \n",
    "    + Missing Value Ratio  \n",
    "    + Low Variance Filter  \n",
    "    + High Correlation Filter  \n",
    "    + Random Forest  \n",
    "    + Backward Feature Elimination  \n",
    "    + Forward Feature Selection  \n",
    "    + Factor Analysis  \n",
    "    + Principal Component Analysis  \n",
    "    + Independent Component Analysis  \n",
    "    + Methods Based on Projections  \n",
    "    + t-Distributed Stochastic Neighbor Embedding (t-SNE)  \n",
    "    + UMAP  \n",
    "+ Applications of Various Dimensionality Reduction Techniques  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is Dimensionality Reduction?\n",
    "\n",
    "We are generating a tremendous amount of data daily. In fact, 90% of the data in the world has been generated in the last 3-4 years! The numbers are truly mind boggling. Below are just some of the examples of the kind of data being collected:\n",
    "\n",
    "+ Facebook collects data of what you like, share, post, places you visit, restaurants you like, etc.\n",
    "+ Your smartphone apps collect a lot of personal information about you\n",
    "+ Amazon collects data of what you buy, view, click, etc. on their site\n",
    "+ Casinos keep a track of every move each customer makes\n",
    "\n",
    "As data generation and collection keeps increasing, visualizing it and drawing inferences becomes more and more challenging. One of the most common ways of doing visualization is through charts. Suppose we have 2 variables, Age and Height. We can use a scatter or line plot between Age and Height and visualize their relationship easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider a case in which we have, say 100 variables (p=100). In this case, we can have 100(100-1)/2 = 5000 different plots. It does not make much sense to visualize each of them separately, right? In such cases where we have a large number of variables, it is better to select a subset of these variables (p<<100) which captures as much information as the original set of variables.\n",
    "\n",
    "Let us understand this with a simple example. Consider the below image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have weights of similar objects in Kg (X1) and Pound (X2). If we use both of these variables, they will convey similar information. So, it would make sense to use only one variable. We can convert the data from 2D (X1 and X2) to 1D (Y1) as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename='../datasets/Figs/straight_line.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can reduce p dimensions of the data into a subset of k dimensions (k<<n). This is called dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Why is Dimensionality Reduction required?\n",
    "\n",
    "Here are some of the benefits of applying dimensionality reduction to a dataset:\n",
    "\n",
    "+ Space required to store the data is reduced as the number of dimensions comes down\n",
    "+ Less dimensions lead to less computation/training time\n",
    "+ Some algorithms do not perform well when we have a large dimensions. So reducing these dimensions needs to happen for the algorithm to be useful\n",
    "+ It takes care of multicollinearity by removing redundant features. For example, you have two variables – ‘time spent on treadmill in minutes’ and ‘calories burnt’. These variables are highly correlated as the more time you spend running on a treadmill, the more calories you will burn. Hence, there is no point in storing both as just one of them does what you require\n",
    "+ It helps in visualizing data. As discussed earlier, it is very difficult to visualize data in higher dimensions so reducing our space to 2D or 3D may allow us to plot and observe patterns more clearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Common Dimensionality Reduction Techniques\n",
    "\n",
    "\n",
    "We will be using [this](https://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii/) dataset.\n",
    "\n",
    "Dimensionality reduction can be done in two different ways:\n",
    "\n",
    "+ Feature Selection - By only keeping the most relevant variables from the original dataset\n",
    "+ Dimensionality Reduction - By finding a smaller set of new variables, each being a combination of the input variables, containing basically the same information as the input variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Missing Value Ratio\n",
    "\n",
    "Suppose you’re given a dataset. What would be your first step? You would naturally want to explore the data first before building model. While exploring the data, you find that your dataset has some missing values. Now what? You will try to find out the reason for these missing values and then impute them or drop the variables entirely which have missing values (using appropriate methods).\n",
    "\n",
    "What if we have too many missing values (say more than 50%)? Should we impute the missing values or drop the variable? I would prefer to drop the variable since it will not have much information. However, this isn’t set in stone. We can set a threshold value and if the percentage of missing values in any variable is more than that threshold, we will drop the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(datapath,'CSVs/BigMart_Train.csv'))\n",
    "df_test = pd.read_csv(os.path.join(datapath,'CSVs/BigMart_Test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Identifier               0.000000\n",
       "Item_Weight                  17.165317\n",
       "Item_Fat_Content              0.000000\n",
       "Item_Visibility               0.000000\n",
       "Item_Type                     0.000000\n",
       "Item_MRP                      0.000000\n",
       "Outlet_Identifier             0.000000\n",
       "Outlet_Establishment_Year     0.000000\n",
       "Outlet_Size                  28.276428\n",
       "Outlet_Location_Type          0.000000\n",
       "Outlet_Type                   0.000000\n",
       "Item_Outlet_Sales             0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the percentage of missing values in each variable\n",
    "df_train.isnull().sum()/len(df_train)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Identifier               0.000000\n",
       "Item_Weight                  17.165317\n",
       "Item_Fat_Content              0.000000\n",
       "Item_Visibility               0.000000\n",
       "Item_Type                     0.000000\n",
       "Item_MRP                      0.000000\n",
       "Outlet_Identifier             0.000000\n",
       "Outlet_Establishment_Year     0.000000\n",
       "Outlet_Location_Type          0.000000\n",
       "Outlet_Type                   0.000000\n",
       "Item_Outlet_Sales             0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving missing values in a variable\n",
    "a = df_train.isnull().sum()/len(df_train)*100\n",
    "\n",
    "# saving column names in a variable\n",
    "variables = df_train.columns\n",
    "variable = [ ]\n",
    "for i in range(0,12):\n",
    "    if a[i]<=20:   #setting the threshold as 20%\n",
    "        variable.append(variables[i])\n",
    "\n",
    "df_train[variable].isnull().sum()/len(df_train)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
